"""
This script runs a syllabus for class incremental learning on CIFAR100, using a pre-generated HDF5 datafile.

This HDF5 file can be generated by running

    python l2datakit/get_cifar100.py [--tar_file_loc path/to/cifar-100-python.tar.gz]

(See README or actual code for more information on use.)

The L2DATA environment variable should be set before running this script (see datasets_learnkit README), which specifies
the top-level folder under which to look for the pre-generated data. If not set, the default is 'l2data' under the root
directory, e.g. ~/l2data on Linux/Mac.
"""

import matplotlib
matplotlib.use('Agg')
# above 2 lines set the matplotlib backend to 'Agg', which
#  enables matplotlib-plots to also be generated if no X-server
#  is defined (e.g., when running in basic Docker-container)
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

import os
import learnkit
import numpy as np
import torch
import logging
import copy

from torch import optim
from learnkit.utils import module_relative_file

from models.classifier import Classifier
from models.bir import IntegratedReplayModel
import utils



def open_pdf(full_path):
    return PdfPages(full_path)

def get_target_name(task_spaces):
    """Return name of target."""
    outputs = task_spaces[list(task_spaces.keys())[0]]['outputs']
    return list(outputs.keys())[0]

def get_output_dim(task_spaces):
    """Return number of classes."""
    outputs = task_spaces[list(task_spaces.keys())[0]]['outputs']
    label_type = list(outputs.keys())[0]
    return outputs[label_type].n

def reshape_batch(flattened_images):
    """Reshape a batch of 1d flattened images into 3d tensors of (channel, height, width)"""
    batch_size = len(flattened_images)
    red = flattened_images[:, :1024].reshape((batch_size, 32, 32, 1))
    green = flattened_images[:, 1024:2048].reshape((batch_size, 32, 32, 1))
    blue = flattened_images[:, 2048:].reshape((batch_size, 32, 32, 1))
    return np.concatenate((red, green, blue), axis=-1).transpose((0, 3, 1, 2))

## Function for (re-)initializing the parameters of [model]
def init_params(model, args):
    # - reinitialize all parameters according to default initialization
    model.apply(utils.weight_reset)
    # - initialize parameters according to chosen custom initialization (if requested)
    if hasattr(args, 'init_weight') and not args.init_weight=="standard":
        utils.weight_init(model, strategy="xavier_normal")
    if hasattr(args, 'init_bias') and not args.init_bias=="standard":
        utils.bias_init(model, strategy="constant", value=0.01)
    # - use pre-trained weights in conv-layers
    load_name = "{}-e100".format(model.convE.name)
    utils.load_checkpoint(model.convE, model_dir='./conv_layers', name=load_name)
    # - freeze weights of conv-layers?
    if utils.checkattr(args, "bir"):
        for param in model.convE.parameters():
            param.requires_grad = False
    return model




def train_model(path_to_syllabus):
    """Load syllabus, train model and return the state_dict."""

    cl = learnkit.Classroom()

    # Set amount of logging
    cl.performance_logging = args.logging
    logger = cl._logging.getLogger()
    logger.setLevel(logging.DEBUG if args.debug else logging.INFO)

    # Set device to be used ('cpu' or 'cuda')
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("GPU training is{}available.".format(" " if torch.cuda.is_available() else " *NOT* "))

    # Open syllabus
    with cl.load(path_to_syllabus) as syllabus:
        syllabus.reset()

        # Define Model
        target_name = get_target_name(syllabus.task_spaces)
        total_classes = get_output_dim(syllabus.task_spaces)
        if args.bir:
            model = IntegratedReplayModel(32, 3, total_classes, target_name, only_active=args.only_active,
                                          depth=5, device=device).to(device)
        else:
            model = Classifier(32, 3, total_classes, target_name, only_active=args.only_active, depth=5).to(device)
        print('\n')
        utils.print_model_info(model, title="MODEL")

        # Initialize / use pre-trained / freeze model-parameters
        model = init_params(model, args)
        #--> all models use convolutional layers that we pretrained on CIFAR-10; note that this could instead be
        #     incorporated into the syllabus by adding CIFAR-10 to the start of the syllabus, but that we haven't done
        #     this yet as the CIFAR-10 dataset is not yet available in learnkit.

        # Set optimizer
        model.optim_list = [
            {'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': 0.001},
        ]
        model.optimizer = optim.Adam(model.optim_list, betas=(0.9, 0.999))

        # Set params for Synpatic Intelligence (SI) and register starting values
        if args.si:
            model.si_c = args.si_c
            model.si_param_dict = {'convE': model.convE, 'fcE': model.fcE, 'classifier': model.classifier}
            model.register_start_values()

        # If requested, keep track of training-loss and -accuracy
        if args.plot:
            train_loss = []
            cur_loss = []
            replay_loss = []
            si_loss = []
            train_accuracies = []

        # Loop through all datasets in the syllabus
        task_id = 0
        new_episode = True
        for dataset in syllabus.datasets():
            done = False     # keeps track of when last batch of this dataset has arrived
            task_id += 1

            # Access and print information about the dataset.
            print("\n\nTASK {}:".format(task_id))
            dataset.reset()  # initializes the dataset with the parameters specified in the syllabus
            print("Task-name: {}".format(dataset.info.name))
            print("Input-spaces: {}".format(dataset.get_input_spaces()))
            print("Output-spaces: {}".format(dataset.get_output_spaces()))

            # If performing SI, prepare <dicts> to store running importance estimates & param-values at start of episode
            if new_episode and args.si:
                W, p_old = model.initiate_W()

            # Log name of dataset
            if args.logging:
                dataset.log.debug(dataset.info.name)

            # Loop over all batches in this dataset
            while not done:
                # Get next batch
                batch, done, info = dataset.next_batch()

                # Break if all data had already been returned (only happens if batch size evenly divides dataset)
                if not batch:
                    break

                # Collect batch-ID
                batch_id = batch['id']

                # Reshape x
                #-> x is dict {'cifar_image': np.array of shape (batch_size, 3072)}, each row is flattened 32x32x3 image
                #   (see viz_cifar100.py for a way of reconstructing the original image from the flattened array)
                x = batch['inputs']['cifar_image']
                x = np.interp(reshape_batch(x), [0, 255], [0, 1])
                x = torch.tensor(x, dtype=torch.float).to(device)

                # Perform inference
                #-> y_hat should be a python dictionary with one numpy array of estimates for each key in output_space
                #   (e.g. {'fine_labels': np.array of shape (batch_size,)} )
                tensors_for_weight_update, y_hat = model.forward(x)

                # Collect ground-truth labels and/or log predictions
                #-> y is either - a dict {[target_name]: np.array of shape (batch_size,) }, OR
                #               - None, if no outputs are presented
                y = dataset.get_labels(batch_id, y_hat)

                # If updates are allowed, update the weights (either supervised or unsupervised)
                if dataset.info.enable_updates:
                    if y:
                        ## DO SUPERVISED LEARNING UPDATES (as labels available)
                        # Reshape y
                        y_torch = torch.tensor(y[model.target_name], dtype=torch.long).to(device)
                        # Perform updates
                        loss_dict = model.update_weights(tensors_for_weight_update, y_torch, x=x, rnt=1./task_id)
                        # If performing SI, update running parameter importance estimates in W
                        if args.si:
                            model.update_W(W, p_old)
                        # Log that updates are made
                        if args.logging:
                            dataset.log.debug("    updates enabled")
                        # If requested, print progress on screen
                        if args.verbose:
                            print("Training Loss = {}".format(loss_dict['loss_total']))
                            print("Training Accuracy = {}".format(loss_dict['precision']))
                        # If requested, store training progress for plotting
                        if args.plot:
                            train_loss.append(loss_dict['loss_total'])
                            cur_loss.append(loss_dict['loss_current'])
                            replay_loss.append(loss_dict['loss_replay'])
                            si_loss.append(loss_dict['si_loss'])
                            train_accuracies.append(loss_dict['precision'])
                    else:
                        ## DO UNSUPERVISED LEARNING UPDATES (as no labels available)
                        #--> our brain-like replay model could profit from unsupervised updates, still needs to be added
                        # For now, log that no updates are made due to lack of labels
                        if args.logging:
                            dataset.log.debug(" ***updates not performed as no labels available")
                else:
                    # Log that updates are not allowed
                    if args.logging:
                        dataset.log.debug(" ***updates *DIS*abled")

            # After each episode, perfrom required 'consolidation'-steps for chosen methods
            tasks_per_episode = 100
            new_episode = True if ((task_id-1) % tasks_per_episode)==0 else False
            if new_episode:
                model.previous_model = None  # -> we only need a copy of the last model, discard any previous copies
                # -SI: calculate and update the normalized path integral
                if args.si:
                    model.update_omega(W, model.epsilon)
                # -LwF / BI-R: consolidate current state of model for generating replay in next episode
                if args.lwf or args.bir:
                    model.previous_model = copy.deepcopy(model).eval()

    # If requested, plot training-loss and -accuracy as funstion of # of (supervised) batches trained on
    if args.plot:
        # -open pdf
        param_stamp = "test"
        plot_name = "./plots/{}.pdf".format(param_stamp)
        if not os.path.isdir('./plots'):
            os.mkdir('./plots')
        pp = open_pdf(plot_name)
        figure_list = []
        # -create figure(s)
        fig, (ax1, ax2) = plt.subplots(1, 2)
        ax1.plot(train_loss, label="Total loss")
        ax1.plot(cur_loss, label="Loss on current episode")
        ax1.plot(replay_loss, label="Loss on replayed data")
        ax1.plot(si_loss, label="Loss of 'Synaptic Intelligence'")
        ax1.set_title('Training Loss')
        ax1.set_xlabel('# of batches')
        ax1.legend()
        ax2.plot(train_accuracies)
        ax2.set_title('Training Accuracy')
        ax2.set_xlabel('# of batches')
        figure_list.append(fig)
        # -add figures to pdf
        for figure in figure_list:
            pp.savefig(figure)
        # -close pdf
        pp.close()

    # Return state_dict
    return model.state_dict()



if __name__ == "__main__":
    import argparse
    syllabus = 'syllabi/incremental_cifar100'

    # Parse provided input options
    parser = argparse.ArgumentParser(description="Sample classification task training script.")
    parser.add_argument('--debug', action='store_true')
    parser.add_argument('--logging', action='store_true')
    parser.add_argument('--verbose', action='store_true')
    parser.add_argument('--plot', action='store_true')
    parser.add_argument('--not-only-active', action='store_false', dest='only_active',
                        help='model can also predict/generate classes not yet seen')
    parser.add_argument('--lwf', action='store_true')
    parser.add_argument('--bir', action='store_true')
    parser.add_argument('--si', action='store_true')
    parser.add_argument('--c', type=float, dest="si_c", help="--> SI: regularisation strength")
    args = parser.parse_args()

    # Select default-values
    args.si_c = (100000000. if args.bir else 1.) if (args.si_c is None) else args.si_c

    # Train model on chosen syllabus
    state_dict = train_model(module_relative_file(__file__, syllabus))
